{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, line_search, BFGS, SR1\n",
    "from numdifftools import Gradient\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescentAlgorithm:\n",
    "    \n",
    "    def __init__(self, fun, gradient=None, hess=None, nd={},\n",
    "                 wolfe_c1=1e-4, wolfe_c2=0.1, x_tol=1e-6,\n",
    "                 f_tol=1e-6, max_iter=50, save_history=False):\n",
    "        \n",
    "        self.fun = fun\n",
    "        \n",
    "        if gradient is None:\n",
    "            self.gradient = Gradient(fun, **nd)\n",
    "        else:\n",
    "            self.gradient = gradient\n",
    "        \n",
    "        self.hess = hess\n",
    "        self.wolfe_coefs = wolfe_c1, wolfe_c2\n",
    "        self.x_tol = x_tol\n",
    "        self.f_tol = f_tol\n",
    "        self.max_iter = max_iter\n",
    "        self.save_history = save_history\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, x0, *args, **kwargs):\n",
    "        \n",
    "        x0 = np.atleast_1d(x0).astype(float)\n",
    "        self.history = []\n",
    "        xk = x0.copy()\n",
    "        fk = self.fun(x0, *args, **kwargs)\n",
    "        gradk = self.gradient(x0, *args, **kwargs)\n",
    "        \n",
    "        fc, gc = 1, 1\n",
    "        \n",
    "        pk = self.prepare_initial_step(xk, fk, gradk, *args,\n",
    "                                       **kwargs)\n",
    "        \n",
    "        advance_x, advance_f, advance_max = True, True, True\n",
    "        k=0\n",
    "        sk = 0\n",
    "        \n",
    "        if self.save_history:\n",
    "            self.history.append({\"x\":xk, \"f\":fk, \"grad\":gradk})\n",
    "        \n",
    "        while (advance_x or advance_f) and (k <= self.max_iter):\n",
    "            \n",
    "            alpha, fc_, gc_, fnew, fk, gradnew \\\n",
    "                = line_search(self.fun, self.gradient,\n",
    "                              xk, sk, gradk, fk, args=args,\n",
    "                              c1=self.wolfe_coefs[0],\n",
    "                              c2=self.wolfe_coefs[1],\n",
    "                              maxiter=15)\n",
    "            \n",
    "            if alpha is None:\n",
    "                alpha = 1\n",
    "                fnew = self.fun(xk + alpha * sk, *args, **kwargs)\n",
    "                gradnew = self.gradient(xk + alpha * sk, *args,\n",
    "                                        **kwargs)\n",
    "            \n",
    "            xnew = xk + alpha * pk\n",
    "            fc = fc + fc_\n",
    "            gc = gc + gc_\n",
    "            \n",
    "            if gradnew is None:\n",
    "                gradnew = self.gradient(xnew, *args, **kwargs)\n",
    "            \n",
    "            advance_f = abs(fnew - fk) > self.f_tol\n",
    "            advance_x = np.linalg.norm(xnew - xk) > self.x_tol\n",
    "            \n",
    "            xk, fk, gradk, pk = \\\n",
    "                self.prepare_next_step(xk, fk, gradk, pk,\n",
    "                                       xnew, fnew, gradnew,\n",
    "                                       *args, **kwargs)\n",
    "            k = k + 1\n",
    "            \n",
    "            if self.save_history:\n",
    "                self.history.append({\"x\":xk, \"f\":fk, \"grad\":gradk})\n",
    "            \n",
    "            if np.linalg.norm(pk) < np.sqrt(np.finfo(float).eps):\n",
    "                self.message = 'Negligible step'\n",
    "                self.success = True\n",
    "                break\n",
    "        \n",
    "        if not (advance_x or advance_f):\n",
    "            self.success = True\n",
    "            self.message = 'Tolerance reached'\n",
    "            \n",
    "        elif k > self.max_iter:\n",
    "            self.success = False\n",
    "            self.message = 'Max iterations reached'\n",
    "        \n",
    "        self.x = xk\n",
    "        self.f = fk\n",
    "        self.grad = gradk\n",
    "        self.fc = fc\n",
    "        self.gc = gc\n",
    "        self.result = {\"x\":xk, \"f\":fk, \"grad\":gradk, \"iter\":k,\n",
    "                       \"message\":self.message,\n",
    "                       \"success\":self.success}\n",
    "    \n",
    "    def prepare_next_step(self, xk, fk, gradk, pk, xnew, fnew, \n",
    "                          gradnew, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def prepare_initial_step(self, xk, fk, gradk, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescent(DescentAlgorithm):\n",
    "\n",
    "    def prepare_next_step(self, xk, fk, gradk, pk, xnew, fnew,\n",
    "                          gradnew, *args, **kwargs):\n",
    "        return xnew, fnew, gradnew, -gradnew\n",
    "\n",
    "    def prepare_initial_step(self, xk, fk, gradk, *args, **kwargs):\n",
    "        return -gradk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun(x):\n",
    "    return (x[0] - 0.5) ** 2 + 0.7 * x[0] * x[1] \\\n",
    "        + 1.2 * (x[1] + 0.7) ** 2\n",
    "\n",
    "def gradient_fun(x):\n",
    "    return np.array([2 * (x[0] - 0.5) + 0.7 * x[1], \\\n",
    "        0.7 * x[0] + 2 * 1.2 * (x[1] + 0.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton's method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Newton(DescentAlgorithm):\n",
    "\n",
    "    def __init__(self, fun, gradient=None, hess=None, nd={},\n",
    "                 wolfe_c1=1e-4, wolfe_c2=0.9,\n",
    "                 x_tol=1e-6, f_tol=1e-6, max_iter=50,\n",
    "                 save_history=False):\n",
    "\n",
    "\n",
    "        #hess= np.array([[2., 0.7],\n",
    "        #            [0.7, 2. * 1.2]])\n",
    "    \n",
    "        if hess is None:\n",
    "            raise TypeError(\"Must provide hessian\")\n",
    "\n",
    "        super().__init__(fun, gradient=gradient, hess=hess, nd=nd,\n",
    "                         wolfe_c1=wolfe_c1, wolfe_c2=wolfe_c2,\n",
    "                         x_tol=x_tol, f_tol=f_tol,\n",
    "                         max_iter=max_iter,\n",
    "                         save_history=save_history)\n",
    "\n",
    "    def prepare_next_step(self, xk, fk, gradk, pk, xnew, fnew,\n",
    "                          gradnew, *args, **kwargs):\n",
    "        H = self.hess(xnew, *args, **kwargs)\n",
    "        return xnew, fnew, gradnew, np.linalg.solve(H, -gradnew)\n",
    "\n",
    "    def prepare_initial_step(self, xk, fk, gradk, *args, **kwargs):\n",
    "        H = self.hess(xk, *args, **kwargs)\n",
    "        return np.linalg.solve(H, -gradk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x):\n",
    "    return np.array([\n",
    "        -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2),\n",
    "        200*(x[1] - x[0]**2)\n",
    "    ])\n",
    "\n",
    "def rosenbrock_hessian(x):\n",
    "    return np.array([\n",
    "        [2 - 400*x[1] + 1200*x[0]**2, -400*x[0]],\n",
    "        [-400*x[0], 200]\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
